<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ImageTx</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }
    </style>
</head>
<body class="bg-gray-100 min-h-screen flex items-center justify-center">
    <div class="container mx-auto max-w-md p-6 bg-white rounded-xl shadow-lg">
        <h1 class="text-2xl font-bold mb-4 text-center">ImageText Extractor</h1>
        
        <div id="libraryStatus" class="mb-4 text-center text-yellow-600">
            Loading Libraries...
        </div>
        
        <input 
            type="file" 
            id="imageUpload" 
            accept="image/*" 
            class="mb-4 w-full p-2 border rounded"
            disabled
        >
        
        <div id="imagePreview" class="mb-4 text-center">
            <img id="previewImg" class="max-w-full max-h-64 mx-auto hidden" />
        </div>
        
        <button 
            id="processBtn" 
            class="w-full bg-blue-500 text-white py-2 rounded hover:bg-blue-600 disabled:opacity-50"
            disabled
        >
            Extract
        </button>
        
        <div id="loadingIndicator" class="hidden mt-4 text-center">
            <p class="text-blue-600">Processing image...</p>
        </div>
        
        <div id="resultContainer" class="mt-4 bg-gray-100 p-4 rounded">
            <h2 class="font-bold mb-2">Extracted:</h2>
            <pre id="resultText" class="whitespace-pre-wrap break-words"></pre>
        </div>
    </div>

    <script type="module">
        // Import Transformers directly as a module

        import { AutoProcessor, AutoTokenizer, LlavaOnevisionForConditionalGeneration, RawImage } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.1.0/+esm';

        // DOM Elements
        const libraryStatus = document.getElementById('libraryStatus');
        const imageUpload = document.getElementById('imageUpload');
        const previewImg = document.getElementById('previewImg');
        const processBtn = document.getElementById('processBtn');
        const loadingIndicator = document.getElementById('loadingIndicator');
        const resultText = document.getElementById('resultText');
        const model_id = 'llava-hf/llava-onevision-qwen2-0.5b-ov-hf';

        // State
        let model = null;
        let processor = null;
        let tokenizer = null;
        const prompt = 'you will be presented with some posters containing pricing data for money transfer , read the sender and reciever country and the transfer rates and any offer details in the image. try to read all the text , but dont try and guess the rates , say you couldnot read the text well in case you find some parts of the image unreadable, a tabluar format data output would be nice. now based on this context tell me What is the text in the image?';
        const messages = [
            { role: 'system', content: 'Answer the question.' },
            { role: 'user', content: `<image>\n${prompt}` }
        ]



        // Initialize the OCR pipeline
        async function initPipeline() {
            try {
                libraryStatus.textContent = 'Initializing Model...';
                libraryStatus.classList.remove('text-yellow-600');
                libraryStatus.classList.add('text-blue-600');

                // Initialize pipeline

                // Load processor and model
                tokenizer = await AutoTokenizer.from_pretrained(model_id);
                processor = await AutoProcessor.from_pretrained(model_id);
                model = await LlavaOnevisionForConditionalGeneration.from_pretrained(model_id, {
                dtype: {
                    embed_tokens: 'fp16', // or 'fp32' or 'q8'
                    vision_encoder: 'fp16', // or 'fp32' or 'q8'
                    decoder_model_merged: 'q4', // or 'q8'
                },
                device: 'webgpu',
                });

                
                
                // Enable UI elements
                imageUpload.disabled = false;
                libraryStatus.textContent = 'Model Loaded!!!!';
                libraryStatus.classList.remove('text-blue-600');
                libraryStatus.classList.add('text-green-600');
            } catch (error) {
                console.error('Extraction error:', error);
                libraryStatus.textContent = `Error: ${error.message}`;
                libraryStatus.classList.remove('text-blue-600');
                libraryStatus.classList.add('text-red-600');
            }
        }
        
        let imageUrl = null;
        // Image preview
        imageUpload.addEventListener('change', function(e) {
            const file = e.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = function(e) {
                    const dataURL = e.target.result;
                    previewImg.src = dataURL;
                    previewImg.classList.remove('hidden');
                    processBtn.disabled = false;
                    imageUrl = dataURL;       
                };
                reader.readAsDataURL(file);

            }
        });



        // Process image
        processBtn.addEventListener('click', async function() {
            if (!processor) {
                alert('Extraction process not initialized. Please wait....');
                return;
            }

            // Reset UI
            resultText.textContent = '';
            loadingIndicator.classList.remove('hidden');
            processBtn.disabled = true;

            try {
                const image = await RawImage.fromURL(imageUrl);
                const text = tokenizer.apply_chat_template(messages, { tokenize: false, add_generation_prompt: true });
                const text_inputs = tokenizer(text);

                const vision_inputs = await processor(image);

                // Generate response
                const { past_key_values, sequences } = await model.generate({
                    ...text_inputs,
                    ...vision_inputs,
                    do_sample: false,
                    max_new_tokens: 64,
                    return_dict_in_generate: true,
                });

                // Decode output
                const answer = tokenizer.decode(
                    sequences.slice(0, [text_inputs.input_ids.dims[1], null]),
                    { skip_special_tokens: true },
                );
                console.log(answer);
                
                // Display results
                resultText.textContent = JSON.stringify(answer, null, 2);
            } catch (error) {
                console.error('Processing Error:', error);
                resultText.textContent = `Error: ${error.message}`;
            } finally {
                loadingIndicator.classList.add('hidden');
                processBtn.disabled = false;
            }
        });

        // Initialize pipeline when module loads
        initPipeline();
    </script>
</body>
</html>
